{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "79dab85a92474f5b8d26980fb1369ed2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a60bd978073448b48dcf8cd60635f2b0",
              "IPY_MODEL_724f1ee0f40546df8d2dbdc388b22d8e",
              "IPY_MODEL_3e04617e2ea843e399080b2cba34fa33"
            ],
            "layout": "IPY_MODEL_845a185d868348f1a96edb0badac803a"
          }
        },
        "a60bd978073448b48dcf8cd60635f2b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fca8b2e10221478a839f79431ae53f1e",
            "placeholder": "​",
            "style": "IPY_MODEL_92326fcb54354186af617f97214aa717",
            "value": "Fetching 1 files: 100%"
          }
        },
        "724f1ee0f40546df8d2dbdc388b22d8e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd096008aa3f452593b9b6abdc02aca9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f1b2d58beffe4dd08cfd5c23e838ac28",
            "value": 1
          }
        },
        "3e04617e2ea843e399080b2cba34fa33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_941847ee153946be9ebce4e62e847fad",
            "placeholder": "​",
            "style": "IPY_MODEL_aa7ec0a8d5dc40a8b9ddd60266cfa739",
            "value": " 1/1 [00:00&lt;00:00, 62.32it/s]"
          }
        },
        "845a185d868348f1a96edb0badac803a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fca8b2e10221478a839f79431ae53f1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92326fcb54354186af617f97214aa717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd096008aa3f452593b9b6abdc02aca9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1b2d58beffe4dd08cfd5c23e838ac28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "941847ee153946be9ebce4e62e847fad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa7ec0a8d5dc40a8b9ddd60266cfa739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZIXi0bfLGaB7",
        "outputId": "3adb4c15-3a40-4a60-82b8-f985a20bfad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.12/dist-packages (1.0.3)\n",
            "Requirement already satisfied: mediapipe in /usr/local/lib/python3.12/dist-packages (0.10.14)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.12/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.1.12)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.12/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.12.0.88)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (4.25.8)\n",
            "Requirement already satisfied: sounddevice>=0.4.4 in /usr/local/lib/python3.12/dist-packages (from mediapipe) (0.5.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.12/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (0.5.3)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.12/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->mediapipe) (3.2.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch transformers sentence-transformers scikit-learn pandas opencv-python moviepy mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import mediapipe as mp\n",
        "from moviepy.editor import VideoFileClip\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "JwpcFhZEHVsh"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section initializes and loads the three core AI models that form the backbone of our multimodal system. Each model is responsible for a different modality: speech, vision, and language.\n",
        "1.  **Whisper**: A state-of-the-art speech-to-text model from OpenAI for transcribing spoken words.\n",
        "2.  **MediaPipe Hands**: A computer vision model from Google for detecting hand landmarks in real-time.\n",
        "3.  **Zero-Shot Classifier**: A powerful NLP model (BART) that can classify text into predefined categories (intents) without being explicitly trained on them.\n",
        "Using a GPU (`device=0`) is specified to significantly speed up model inference."
      ],
      "metadata": {
        "id": "OnJ-PykprSnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Speech-to-Text Model (Whisper)\n",
        "# Using a GPU (device=0) is highly recommended for Whisper\n",
        "stt_pipeline = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base.en\", device=0)\n",
        "print(\"--> Whisper Speech-to-Text model loaded.\")\n",
        "\n",
        "# 2. Hand Gesture Model (MediaPipe)\n",
        "mp_hands = mp.solutions.hands\n",
        "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.7)\n",
        "mp_drawing = mp.solutions.drawing_utils\n",
        "print(\"--> MediaPipe Hand Gesture model loaded.\")\n",
        "\n",
        "# 3. ZERO-SHOT TEXT-TO-INTENT NLP Model\n",
        "# We replace our custom classifier with a powerful pre-trained model.\n",
        "# facebook/bart-large-mnli is a popular choice for this task.\n",
        "zero_shot_classifier = pipeline(\"zero-shot-classification\", model=\"facebook/bart-large-mnli\", device=0)\n",
        "# Define our possible intents which will be the candidate labels\n",
        "CANDIDATE_INTENTS = [\"forward\", \"left\", \"right\", \"stop\"]\n",
        "print(\"--> Zero-Shot Intent NLP model loaded.\")\n",
        "print(\"\\n\" + \"=\"*50 + \"\\nAll models are ready.\\n\" + \"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "79dab85a92474f5b8d26980fb1369ed2",
            "a60bd978073448b48dcf8cd60635f2b0",
            "724f1ee0f40546df8d2dbdc388b22d8e",
            "3e04617e2ea843e399080b2cba34fa33",
            "845a185d868348f1a96edb0badac803a",
            "fca8b2e10221478a839f79431ae53f1e",
            "92326fcb54354186af617f97214aa717",
            "fd096008aa3f452593b9b6abdc02aca9",
            "f1b2d58beffe4dd08cfd5c23e838ac28",
            "941847ee153946be9ebce4e62e847fad",
            "aa7ec0a8d5dc40a8b9ddd60266cfa739"
          ]
        },
        "collapsed": true,
        "id": "986FZ_BmH-ha",
        "outputId": "b7d32440-f300-4697-bdae-0568c7bfe208"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 1 files:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79dab85a92474f5b8d26980fb1369ed2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Whisper Speech-to-Text model loaded.\n",
            "--> MediaPipe Hand Gesture model loaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Zero-Shot Intent NLP model loaded.\n",
            "\n",
            "==================================================\n",
            "All models are ready.\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function takes a string of text (the transcript from the audio) and uses the pre-trained zero-shot classification model to determine which of the `CANDIDATE_INTENTS` it most closely matches. It works \"zero-shot,\" meaning the model was not specifically trained on our \"forward,\" \"left,\" \"right,\" or \"stop\" commands but can generalize to understand them. The function only returns an intent if the model's confidence score exceeds a specified threshold, preventing uncertain classifications."
      ],
      "metadata": {
        "id": "1b5ELIO_stVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_intent_from_video(video_path):\n",
        "    \"\"\"\n",
        "    Analyzes a video for hand gesture SEQUENCE:\n",
        "    First: All 3 directional gestures (left, right, forward) in any order\n",
        "    Then: Stop gesture to complete the sequence\n",
        "    \"\"\"\n",
        "    print(\"\\n[Video] Analyzing video for gesture sequence...\")\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    if not cap.isOpened(): return None\n",
        "\n",
        "    gesture_counts = {\"left\": 0, \"right\": 0, \"forward\": 0, \"stop\": 0, \"unknown\": 0}\n",
        "    gesture_sequence = []  # Track order of gestures detected\n",
        "    frame_count = 0\n",
        "    last_gesture = None\n",
        "    gesture_hold_frames = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret: break\n",
        "\n",
        "        if frame_count % 5 == 0:\n",
        "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            results = hands.process(frame_rgb)\n",
        "\n",
        "            current_gesture = None\n",
        "\n",
        "            if results.multi_hand_landmarks:\n",
        "                for hand_landmarks in results.multi_hand_landmarks:\n",
        "                    # Collect key landmarks\n",
        "                    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
        "                    thumb_ip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_IP]\n",
        "\n",
        "                    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
        "                    index_pip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_PIP]\n",
        "\n",
        "                    middle_tip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n",
        "                    middle_pip = hand_landmarks.landmark[mp_hands.HandLandmark.MIDDLE_FINGER_PIP]\n",
        "\n",
        "                    ring_tip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_TIP]\n",
        "                    ring_pip = hand_landmarks.landmark[mp_hands.HandLandmark.RING_FINGER_PIP]\n",
        "\n",
        "                    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
        "                    pinky_pip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_PIP]\n",
        "\n",
        "                    wrist = hand_landmarks.landmark[mp_hands.HandLandmark.WRIST]\n",
        "\n",
        "                    # 1. Condition for Left/Right Fist\n",
        "                    index_folded = index_tip.y > index_pip.y\n",
        "                    middle_folded = middle_tip.y > middle_pip.y\n",
        "                    ring_folded = ring_tip.y > ring_pip.y\n",
        "                    pinky_folded = pinky_tip.y > pinky_pip.y\n",
        "\n",
        "                    is_fist_with_thumb = (\n",
        "                        index_folded and middle_folded and ring_folded and pinky_folded and\n",
        "                        abs(thumb_tip.y - thumb_ip.y) < 0.05\n",
        "                    )\n",
        "\n",
        "                    # 2. Condition for Stop (Open Palm)\n",
        "                    fingers_open = (\n",
        "                        index_tip.y < index_pip.y and\n",
        "                        middle_tip.y < middle_pip.y and\n",
        "                        ring_tip.y < ring_pip.y and\n",
        "                        pinky_tip.y < pinky_pip.y and\n",
        "                        thumb_tip.y < thumb_ip.y\n",
        "                    )\n",
        "\n",
        "                    # 3. Condition for Forward (Thumbs Up)\n",
        "                    is_thumbs_up = (\n",
        "                        thumb_tip.y < thumb_ip.y - 0.03 and\n",
        "                        index_folded and middle_folded and ring_folded and pinky_folded\n",
        "                    )\n",
        "\n",
        "                    # PRIORITY 1: Check for Left/Right Fist\n",
        "                    if is_fist_with_thumb:\n",
        "                        if thumb_tip.x < wrist.x - 0.04:\n",
        "                            current_gesture = \"left\"\n",
        "                        elif thumb_tip.x > wrist.x + 0.04:\n",
        "                            current_gesture = \"right\"\n",
        "                        else:\n",
        "                            if is_thumbs_up:\n",
        "                                current_gesture = \"forward\"\n",
        "\n",
        "                    # PRIORITY 2: Check for Stop (Open Palm)\n",
        "                    elif fingers_open:\n",
        "                        current_gesture = \"stop\"\n",
        "\n",
        "                    # PRIORITY 3: Check for Forward (Thumbs Up)\n",
        "                    elif is_thumbs_up:\n",
        "                        current_gesture = \"forward\"\n",
        "\n",
        "            # Track gesture stability - REDUCED STABILITY REQUIREMENT\n",
        "            if current_gesture == last_gesture:\n",
        "                gesture_hold_frames += 1\n",
        "            else:\n",
        "                gesture_hold_frames = 0\n",
        "                last_gesture = current_gesture\n",
        "\n",
        "            # Add gesture to sequence if held for 2+ frames (less strict)\n",
        "            if current_gesture and gesture_hold_frames >= 2:\n",
        "                if len(gesture_sequence) == 0 or gesture_sequence[-1] != current_gesture:\n",
        "                    gesture_sequence.append(current_gesture)\n",
        "                    gesture_counts[current_gesture] += 1\n",
        "                    print(f\"[Video] Detected: {current_gesture} (Sequence: {gesture_sequence})\")\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "    # Check if we have a valid sequence\n",
        "    print(f\"[Video] Final Gesture Sequence: {gesture_sequence}\")\n",
        "    print(f\"[Video] Gesture Counts: {gesture_counts}\")\n",
        "\n",
        "    # Valid sequence: contains left, right, forward (in any order) + ends with stop\n",
        "    required_gestures = {\"left\", \"right\", \"forward\"}\n",
        "    detected_gestures = set(gesture_sequence)\n",
        "\n",
        "    # More lenient completion check - just need all 3 directional + stop somewhere\n",
        "    has_all_directional = required_gestures.issubset(detected_gestures)\n",
        "    has_stop = \"stop\" in detected_gestures\n",
        "\n",
        "    if has_all_directional and has_stop:\n",
        "        print(\"[Video] SUCCESS: Valid gesture sequence detected!\")\n",
        "        return \"sequence_complete\"\n",
        "    elif has_all_directional and not has_stop:\n",
        "        print(\"[Video] PARTIAL: All directional gestures detected, need STOP...\")\n",
        "        return \"sequence_partial\"\n",
        "    else:\n",
        "        missing = required_gestures - detected_gestures\n",
        "        print(f\"[Video] INCOMPLETE: Missing gestures: {missing}\")\n",
        "        if not has_stop:\n",
        "            print(\"[Video] Also missing: STOP gesture\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def process_multimodal_command(video_path):\n",
        "    \"\"\"\n",
        "    Updated pipeline for sequence-based commands with audio+video\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*20} PROCESSING SEQUENCE COMMAND: {video_path} {'='*20}\")\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Error: Video file not found at {video_path}\"); return\n",
        "\n",
        "    # --- Step 1: Extract Audio & Get Intents ---\n",
        "    temp_audio_path = \"temp_audio.wav\"\n",
        "    audio_commands = []  # Track all audio commands detected\n",
        "\n",
        "    try:\n",
        "        with VideoFileClip(video_path) as video_clip:\n",
        "            video_clip.audio.write_audiofile(temp_audio_path, logger=None)\n",
        "\n",
        "        # You might want to modify get_intent_from_audio to return all detected commands\n",
        "        # For now, assuming it returns the most dominant command\n",
        "        audio_intent = get_intent_from_audio(temp_audio_path)\n",
        "\n",
        "    except Exception:\n",
        "        audio_intent = None\n",
        "    finally:\n",
        "        if os.path.exists(temp_audio_path): os.remove(temp_audio_path)\n",
        "\n",
        "    video_intent = get_intent_from_video(video_path)\n",
        "\n",
        "    # --- Step 2: SEQUENCE DECISION LOGIC ---\n",
        "    print(\"\\n[Fusion] Analyzing sequence completion...\")\n",
        "    print(f\"[Fusion] Audio Intent: {audio_intent} | Video Intent: {video_intent}\")\n",
        "\n",
        "    # Case 1: Complete sequence detected\n",
        "    if video_intent == \"sequence_complete\":\n",
        "        if audio_intent:\n",
        "            print(f\"\\nSEQUENCE COMPLETE: Video sequence finished with audio confirmation!\")\n",
        "            print(\"Executing full command sequence...\")\n",
        "            # Execute your robot sequence here\n",
        "        else:\n",
        "            print(f\"\\nSEQUENCE COMPLETE: Video sequence finished (no audio detected)\")\n",
        "            print(\"Executing full command sequence...\")\n",
        "\n",
        "    # Case 2: Partial sequence\n",
        "    elif video_intent == \"sequence_partial\":\n",
        "        print(f\"\\nSEQUENCE PARTIAL: Directional gestures complete, perform STOP gesture to finish\")\n",
        "\n",
        "    # Case 3: Audio only (fallback to original logic)\n",
        "    elif audio_intent and not video_intent:\n",
        "        print(f\"\\nAUDIO ONLY: Single command detected: {audio_intent.upper()}\")\n",
        "\n",
        "    # Case 4: No clear sequence\n",
        "    else:\n",
        "        print(f\"\\nINCOMPLETE: Sequence not detected. Please perform all gestures (left, right, forward) then stop.\")"
      ],
      "metadata": {
        "id": "UAQMiZKcIjyN"
      },
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It takes the path to an audio file, uses the Whisper model to transcribe the speech into text, and then passes this text to our `get_intent_from_text_zero_shot` function to determine the final command intent. It includes error handling in case the audio processing fails."
      ],
      "metadata": {
        "id": "nZgk6U5ss6EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For a finger to be curled, its tip must be \"lower\" on the screen than its middle joint (the PIP joint). In screen coordinates, a higher y value means lower on the screen. This condition checks if the main fingers are bent downwards."
      ],
      "metadata": {
        "id": "NK3ijjdg7Feq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the core function that combines the entire multimodal analysis. It takes a video file path as input and performs the following steps:\n",
        "1.  Extracts the audio from the video into a temporary file.\n",
        "2.  Runs the audio processing pipeline to get an `audio_intent`.\n",
        "3.  Runs the video gesture recognition pipeline to get a `video_intent`.\n",
        "4.  Decision"
      ],
      "metadata": {
        "id": "zZLFH7mqtKEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    test_videos = [\n",
        "        \"/content/forward.mov\",\n",
        "    ]\n",
        "\n",
        "    for video_file in test_videos:\n",
        "        process_multimodal_command(video_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ui5Ud_iNIxsm",
        "outputId": "064ac49b-8763-4031-b406-935f58e84978"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== PROCESSING SEQUENCE COMMAND: /content/forward.mov ====================\n",
            "\n",
            "[Audio] Transcribing speech to text...\n",
            "[Audio] Raw Transcription Result: {'text': ' Right. Left.'}\n",
            "[NLP] Classifying text: 'right. left.'\n",
            "[NLP] Top classification: 'forward' with confidence: 0.37\n",
            "[NLP] Confidence is below threshold. Intent is uncertain.\n",
            "\n",
            "[Video] Analyzing video for gesture sequence...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Video] Detected: stop (Sequence: ['stop'])\n",
            "[Video] Detected: forward (Sequence: ['stop', 'forward'])\n",
            "[Video] Detected: stop (Sequence: ['stop', 'forward', 'stop'])\n",
            "[Video] Detected: right (Sequence: ['stop', 'forward', 'stop', 'right'])\n",
            "[Video] Final Gesture Sequence: ['stop', 'forward', 'stop', 'right']\n",
            "[Video] Gesture Counts: {'left': 0, 'right': 1, 'forward': 1, 'stop': 2, 'unknown': 0}\n",
            "[Video] INCOMPLETE: Missing gestures: {'left'}\n",
            "\n",
            "[Fusion] Analyzing sequence completion...\n",
            "[Fusion] Audio Intent: None | Video Intent: None\n",
            "\n",
            "INCOMPLETE: Sequence not detected. Please perform all gestures (left, right, forward) then stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    test_videos = [\n",
        "        \"/content/stop.mov\",\n",
        "    ]\n",
        "\n",
        "    for video_file in test_videos:\n",
        "        process_multimodal_command(video_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNr8ve6EckW6",
        "outputId": "ea4fd0d7-53c6-4e93-bb8d-204e1de37ff1"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== PROCESSING SEQUENCE COMMAND: /content/stop.mov ====================\n",
            "\n",
            "[Audio] Transcribing speech to text...\n",
            "[Audio] Raw Transcription Result: {'text': \" Let's don't.\"}\n",
            "[NLP] Classifying text: 'let's don't.'\n",
            "[NLP] Top classification: 'stop' with confidence: 0.62\n",
            "[NLP] Confidence is above threshold. Intent is 'stop'.\n",
            "\n",
            "[Video] Analyzing video for gesture sequence...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Video] Detected: stop (Sequence: ['stop'])\n",
            "[Video] Final Gesture Sequence: ['stop']\n",
            "[Video] Gesture Counts: {'left': 0, 'right': 0, 'forward': 0, 'stop': 1, 'unknown': 0}\n",
            "[Video] INCOMPLETE: Missing gestures: {'left', 'forward', 'right'}\n",
            "\n",
            "[Fusion] Analyzing sequence completion...\n",
            "[Fusion] Audio Intent: stop | Video Intent: None\n",
            "\n",
            "AUDIO ONLY: Single command detected: STOP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\n",
        "    test_videos = [\n",
        "        \"/content/forward.mov\",\n",
        "    ]\n",
        "\n",
        "    for video_file in test_videos:\n",
        "        process_multimodal_command(video_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2c7u8dOkp_l",
        "outputId": "f677a176-6225-4df1-8b34-e4364564e723"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== PROCESSING SEQUENCE COMMAND: /content/forward.mov ====================\n",
            "\n",
            "[Audio] Transcribing speech to text...\n",
            "[Audio] Raw Transcription Result: {'text': ' Right. Left.'}\n",
            "[NLP] Classifying text: 'right. left.'\n",
            "[NLP] Top classification: 'forward' with confidence: 0.37\n",
            "[NLP] Confidence is below threshold. Intent is uncertain.\n",
            "\n",
            "[Video] Analyzing video for gesture sequence...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:py.warnings:/usr/local/lib/python3.12/dist-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
            "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Video] Detected: stop (Sequence: ['stop'])\n",
            "[Video] Detected: forward (Sequence: ['stop', 'forward'])\n",
            "[Video] Detected: stop (Sequence: ['stop', 'forward', 'stop'])\n",
            "[Video] Detected: right (Sequence: ['stop', 'forward', 'stop', 'right'])\n",
            "[Video] Final Gesture Sequence: ['stop', 'forward', 'stop', 'right']\n",
            "[Video] Gesture Counts: {'left': 0, 'right': 1, 'forward': 1, 'stop': 2, 'unknown': 0}\n",
            "[Video] INCOMPLETE: Missing gestures: {'left'}\n",
            "\n",
            "[Fusion] Analyzing sequence completion...\n",
            "[Fusion] Audio Intent: None | Video Intent: None\n",
            "\n",
            "INCOMPLETE: Sequence not detected. Please perform all gestures (left, right, forward) then stop.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n4AEODB4nAod"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}